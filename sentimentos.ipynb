{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import sys\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the file in read mode\n",
    "with open(\"assets/positive.txt\", 'r', encoding='utf-8') as file:\n",
    "    # Read all the lines of the file into a list\n",
    "    positive_sentences = file.readlines()\n",
    "\n",
    "with open(\"assets/negative.txt\", 'r', encoding='utf-8') as file:\n",
    "    # Read all the lines of the file into a list\n",
    "    negative_sentences = file.readlines()\n",
    "\n",
    "sentences = positive_sentences + negative_sentences\n",
    "\n",
    "\n",
    "feelings = [1]*len(positive_sentences) + [-1]*len(negative_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gusta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gusta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Text Libraries\n",
    "#-----------------------------------\n",
    "\n",
    "# Work with regular expressions\n",
    "import re\n",
    "\n",
    "# Gonna use the NLTK librarie and spaCy to pre-process texts\n",
    "\n",
    "import nltk # Natural Language Toolkit\n",
    "import spacy # Industrial-Strength Natural Language Processing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "swords = nltk.corpus.stopwords.words('portuguese')\n",
    "swords.remove(\"não\")\n",
    "\n",
    "# spaCy - upload processing text models to Portuguese\n",
    "#     Lemmatizing\n",
    "\n",
    "#!python -m spacy download pt_core_news_md \n",
    "#!python -m spacy download pt_core_news_lg \n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Contador de texto do SKLEARN - scikit-learn Machine Learning in Python\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_sentence(sentence):\n",
    "\n",
    "    text = re.sub(r\"[^\\w\\s]\",\" \",sentence)\n",
    "    text = nltk.word_tokenize(text.lower())\n",
    "    text = [word for word in text if word not in swords]\n",
    "    text = \" \".join(text)\n",
    "    doc = nlp(text)\n",
    "    text = [token.lemma_ for token in doc if not token.is_punct]\n",
    "    text = \" \".join(text).lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences_array = []\n",
    "clean_feelings_array = []\n",
    "for sentence, feel in zip(sentences, feelings):\n",
    "    sentece_clean = clear_sentence(sentence)\n",
    "    if sentece_clean not in clean_sentences_array:\n",
    "      clean_sentences_array.append(sentence)\n",
    "      clean_feelings_array.append(feel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['10' '100' '18' ... 'único' 'únicos' 'útil']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(clean_sentences_array)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulário:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bag_of_words = vectorizer.fit_transform(clean_sentences_array)\n",
    "\n",
    "\n",
    "df_bag_of_words = pd.DataFrame(bag_of_words.toarray(), columns=vocab)\n",
    "\n",
    "df_bag_of_words['Frases'] = clean_sentences_array\n",
    "\n",
    "colunas = ['Frases'] + list(vocab)\n",
    "df_bag_of_words = df_bag_of_words[colunas]\n",
    "\n",
    "\n",
    "df_bag_of_words['Sentimentos'] = clean_feelings_array\n",
    "\n",
    "colunas = ['Frases'] + ['Sentimentos'] + list(vocab)\n",
    "df_bag_of_words = df_bag_of_words[colunas]\n",
    "\n",
    "df_bag_of_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " w , c \n",
      "[ 1.98721312e+00 -2.77645498e+00  7.10623224e-01 ...  5.55111512e-17\n",
      "  0.00000000e+00  7.75053976e+00] [0.49561575]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "x_train = bag_of_words #.toarray()\n",
    "y_train = clean_feelings_array\n",
    "\n",
    "model = LinearSVC(loss='hinge',C=1e18, max_iter=10000)\n",
    "\n",
    "svmfit = model.fit(x_train,y_train)\n",
    "\n",
    "w_sklearn = svmfit.coef_[0]\n",
    "\n",
    "c_sklearn = - svmfit.intercept_\n",
    "print(\" w , c \")\n",
    "print(w_sklearn, c_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41,)\n",
      "(41, 5221)\n",
      "(5221, 1)\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_assets/positive.txt\", 'r', encoding='utf-8') as file:\n",
    "    # Read all the lines of the file into a list\n",
    "    test_positive_sentences = file.readlines()\n",
    "\n",
    "with open(\"test_assets/negative.txt\", 'r', encoding='utf-8') as file:\n",
    "    # Read all the lines of the file into a list\n",
    "    test_negative_sentences = file.readlines()\n",
    "\n",
    "test_sentences = test_positive_sentences + test_negative_sentences\n",
    "test_feelings = [+1]*len(test_positive_sentences) + [-1]*len(test_negative_sentences)\n",
    "\n",
    "test_clean_sentences = []\n",
    "for sentence in test_sentences:\n",
    "    test_clean_sentences.append(clear_sentence(sentence))\n",
    "\n",
    "vector_test = vectorizer.transform(test_clean_sentences).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taxa de erro para as frases testes: 14.634146341463415  %\n"
     ]
    }
   ],
   "source": [
    "erros = 0\n",
    "for i in range(len(test_sentences)):\n",
    "  #print()\n",
    "  #print(test_sentences[i])\n",
    "  x = vector_test[i].reshape(len(w_sklearn),1)\n",
    "  classific = model.decision_function(x.T) #np.dot(w_sklearn.T, x) - c_sklearn\n",
    "  if classific > 0 :\n",
    "    #print(\" Classificação original :\", (\"Positiva\" if test_feelings[i]> 0 else \"Negativa\") )\n",
    "    #print(\" Modelo classificou como: Positiva! wx-c:\", np.dot(w_sklearn.T, x) - c_sklearn, model.decision_function(x.T))\n",
    "    if test_feelings[i] < 0:\n",
    "      erros = erros +1\n",
    "  else:\n",
    "    #print(\" Classificação original :\", (\"Positiva\" if test_feelings[i]> 0 else \"Negativa\") )\n",
    "    #print(\" Modelo classificou como: Negativa! w.Tx-c:\", np.dot(w_sklearn.T, x) - c_sklearn, model.decision_function(x.T))\n",
    "    if test_feelings[i] > 0:\n",
    "      erros = erros +1\n",
    "\n",
    "print()\n",
    "print(\"Taxa de erro para as frases testes:\", 100*float(erros) / len(test_sentences), \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuar = True\n",
    "while continuar:\n",
    "    chute = input(\"Teste uma frase para avaliar o sentimento: \")\n",
    "    clean = clear_sentence(chute)\n",
    "    format = [clean]\n",
    "    vector = vectorizer.transform(format).toarray()\n",
    "    print(vector.shape)\n",
    "    x = vector.reshape(len(w_sklearn),1)\n",
    "    classific = model.decision_function(x.T) #np.dot(w_sklearn.T, x) - c_sklearn\n",
    "    if classific > 0:\n",
    "        print(\"O modelo classificou como uma frase positiva!\")\n",
    "    else:\n",
    "        print(\"O modelo classificou como uma frase negativa!\")\n",
    "    wanna_cont = input(\"Quer continuar? (s/n)\")\n",
    "    if wanna_cont == 'n' or wanna_cont != 's':\n",
    "        continuar = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
